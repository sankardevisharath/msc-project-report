%%%%%%%%%%%%%%%%%%%%% RJT TeX Template %%%%%%%%%%%%%%%%%%%%%
\documentclass[twoside,11pt,a4paper]{article}

%%%% BHAM PREAMBLE - SET THIS FIRST! %%%%
\newcommand{\bhamstudentname}{Sarathkumar Padinjare Marath Sankaranarayanan}
\newcommand{\bhamthesistitle}{Machine Learning \& Deep Learning Approaches to Predict Credit Card Default}
\newcommand{\bhamfronttitle}{Machine Learning \& Deep Learning Approaches \\ to Predict Credit Card Default}
\newcommand{\bhamschool}{School of Computer Science}
\newcommand{\bhamcollege}{Engineering and Physical Sciences}
\newcommand{\bhamdegree}{MSc. Artificial Intelligence \& Computer Science}
\newcommand{\bhamid}{2359859}
\newcommand{\bhamsupervisor}{Dr.Kashif Rajpoot}
\newcommand{\bhamyear}{2022}
%%%%           %%%%


\usepackage[hyphens]{url}
\usepackage[breaklinks]{hyperref}
\usepackage{fancyhdr}
\usepackage[sort]{natbib} 
\usepackage{comment} % from http://www.latex-community.org/forum/viewtopic.php?f=5&t=4538
\usepackage{dirtree} % from http://blog.plenz.com/2011-07/represent-directory-structures-in-latex.html
\usepackage{longtable} % from http://stackoverflow.com/questions/2896833/how-to-stretch-a-table-over-multiple-pages
\usepackage{algorithm}   
\usepackage{algorithmic}   %both algorithm* from http://hasini-gunasinghe.blogspot.co.uk/2014/02/presenting-algorithmsprotocols-in-neat.html

\renewcommand{\algorithmiccomment}[1]{#1} %http://tex.stackexchange.com/q1uestions/61861/algorithmic-package-for-loop-and-comment-at-the-same-line
\usepackage[printonlyused]{acronym}

\pagestyle{fancy}
\renewcommand{\sectionmark}[1]{\markboth{#1}{}}	%from tex.stackexchange.com/questions/111361

\lfoot{\bhamstudentname}
\cfoot{}
\rfoot{}
\fancyhfoffset[L]{0cm} %this fixes the right page number margin issue.
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\renewcommand{\headrulewidth}{0pt}
\newcommand{\tab}{\hspace*{1.25em}}
\newcommand{\minitab}{\hspace*{0.25em}}

%footnote stuff
\usepackage{perpage}
\MakePerPage{footnote} %the perpage package command
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\lhead{}\chead{}\rhead{}
\setlength{\headheight}{28pt} %fixes the warnings about headheight being too small
\setlength{\headsep}{6pt}
\pdfoutput=1 % we are running PDFLaTeX
\usepackage[left=2.55cm,right=1.6cm,top=1.8cm,bottom=1.8cm]{geometry}
\usepackage{titling}	
\setlength{\droptitle}{-2.75cm}   % This is your set screw\\
\usepackage{titlesec}
\titleformat*{\section}{\normalsize	\bfseries}
\titleformat*{\subsection}{\small \bfseries}
\titleformat*{\subsubsection}{\footnotesize \bfseries}
%modifies the size of the gaps between the top of the title and bottom
% arguments {type}{left}{top}{bottom}
\titlespacing*{\section} {0pt}{3ex plus 1ex minus .2ex}{2ex plus .2ex}
\titlespacing*{\subsection} {0pt}{2.25ex plus 1ex minus .2ex}{0.75ex plus .2ex}
\titlespacing*{\subsubsection}{0pt}{2.ex plus 1ex minus .2ex}{0.5ex plus .2ex}

\setlength{\intextsep}{0pt} %from http://tex.stackexchange.com/questions/25828/how-to-remove-change-the-vertical-spacing-before-and-after-an-algorithm-environ

\usepackage[pdftex]{graphicx}
\graphicspath{ {figures/} }

\usepackage{enumitem}
\usepackage{pdfpages}
\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}


\usepackage{epstopdf} %http://dirkraffel.com/2007/11/19/include-eps-files-in-latex/comment-page-2/

\usepackage{listings}
\lstset{
 basicstyle=\ttfamily,
  columns=fullflexible,
  keepspaces=true,
breaklines=true
} %from http://tex.stackexchange.com/questions/121601/automatically-wrap-the-text-in-verbatim

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{TODO found: #1!}} %from http://tex.stackexchange.com/questions/9796/how-to-add-todo-notes

\DeclareGraphicsExtensions{.jpg,.png}
%%%%%%%%%%%%%%%%%%%%%  END of TEMPLATE %%%%%%%%%%%%%%%%%%%%%

\title{MSc. Project\\\bhamthesistitle}
\author{\textsf{\bhamstudentname {\textsf{}}}}

\date{}
\begin{document}

\pagenumbering{gobble} % fix at	 http://tex.stackexchange.com/questions/7355/how-to-suppress-page-number
% this came from http://en.wikibooks.org/wiki/LaTeX/Title_Creation and http://tex.stackexchange.com/questions/14778/error-with-hrule
\begin{titlepage}
\begin{center}
% this was from http://tex.stackexchange.com/questions/7219/how-to-vertically-center-two-images-next-to-each-other
\begin{minipage}{6in}
  \centering
  \raisebox{-0.5\height}{\includegraphics[width=1.25in]{crest}}
  \hspace*{.2in}
  \raisebox{-0.5\height}{\includegraphics[height=0.9375in]{uni}}
  \end{minipage}
  \\ [1.0cm]
\textsc{{\LARGE \bhamschool\\}College of \bhamcollege}\\[3.5cm]

\textsc{\Large MSc. Project}\\[0.5cm]

% Title
\HRule \\[0.4cm]
\begin{center}\Huge
\bhamfronttitle
\end{center}
\HRule \\[1.5cm]
% Team and Members

\begin{center}
Submitted in conformity with the requirements\\ for the degree of \bhamdegree\\
\bhamschool\\ University of Birmingham\\
\vspace{2cm}
\bhamstudentname \\
Student ID: \bhamid\\
Supervisor: \bhamsupervisor      
\end{center}
\vfill

% Bottom of the page
{\large September \bhamyear}

\end{center}
\end{titlepage}

\section*{\centering Abstract}	


% Taken from the MSc Thesis template, and edited for a PGT report
The material contained within this report has not previously been
submitted for a degree at the University of Birmingham or any other university.
The research reported within this report has been conducted by the author
unless indicated otherwise.\\
\\
\textbf{Keywords} Credit Card Default Prediction, Ensemble Learning

\vfill
\clearpage

\section*{\centering Declaration}
% Taken from the MSc Thesis template, and edited for a PGT report
The material contained within this report has not previously been
submitted for a degree at the University of Birmingham or any other university.
The research reported within this report has been conducted by the author
unless indicated otherwise.\\
\\
\textbf{Signed} Sarathkumar Padinjare Marath Sankaranarayanan 

\vfill
\clearpage
\begin{center}
\vspace*{\fill}
\begin{minipage}{6in}

%\centering \Large{``Most men who have really lived have had, in some share, their great adventure.\\This railway is mine."}\\{\normalsize{\textsc{James J. Hill}}, \emph{Railway Pioneer}} \vspace{2cm}

%\centering \Large{``Steam engines don't answer back.\\ You can belt them with a hammer and they say nowt."}\\{\normalsize{\textsc{Fred Dibnah}}, \emph{Steeplejack and Engineer}} \vspace{2cm}

\centering \Large{``You have to learn the rules of the game.\\ And then you have to play better than anyone else"}\\{\normalsize{\textsc{Albert Einstein}}}

  \end{minipage}
  \vspace*{\fill}
\end{center}


\clearpage
\maketitle
\vspace{-5.5em} %fixes distance between \maketitle and the TOC
\begingroup
    \fontsize{9pt}{11pt}\selectfont
\tableofcontents
\endgroup
\clearpage
\phantomsection
\section*{Table of Abbreviations}

\input{acronyms}

\addcontentsline{toc}{section}{Table of Abbreviations} 
\clearpage


\listoffigures

\addcontentsline{toc}{section}{List of Figures} 
\clearpage

% set up the page numbering and counter - Table of Abbreviations has no page number
% also set up the footers and headers appropriately.
\pagenumbering{arabic}
\setcounter{page}{1}
\lhead{}\chead{MSc. Project Report :: \nouppercase{Section \thesection\minitab :: \leftmark}}\rhead{}
\rfoot{Page \thepage \hspace*{0.2pt} of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}

\section{Introduction}
This section will introduce the user to definitions of terms relevant for understanding the problem, discuss the motivation behind the problem, the aim \& approach taken to solve the problem, and the structure of this report. 


\subsection{Definitions}

\subsubsection{Credit Card Statement Date}
The credit card statement date is the date on which the statement/bill is generated every month. Any transaction conducted on the card post billing date will reflect in the next month's credit card statement.

\subsubsection{Delinquent Account}
A credit card account is considered delinquent if the customer has failed to make the minimum monthly payment for 30 days from the original due date.

\subsubsection{Delinquency Rate}
The percentage of credit card accounts within a financial institution's portfolio whose payments are delinquent.
\begin{equation}
	Delinquency Rate = \left(\frac{Number Of Delinquent Credit Card Accounts}{Total Number Of Credit Card Account}\right) * 100
\end{equation}

\subsubsection{Credit Card Default}
The customer is considered as defaulting customer in the event of nonpayment of the due amount in 120 days after the latest statement date.

\subsection{Motivation}
Delinquency rates \& credit card default rates are directly proportional. According to the  figure \ref{fig:fredgraph}, the delinquency rates were at an all-time high just before the recession started in 2008; moreover, this was the same time when more \& more customers began to default on credit card payments. 

Predicting credit defaults is essential for managing risk in the consumer lending industry. Credit default prediction enables lenders to make the best possible lending decisions, improving customer satisfaction and fostering strong company economics. \\

\begin{figure}[ht]
	\centering
	\includegraphics[width=1.0\textwidth]{fredgraph}
	\caption[Delinquency rate on credit card loans for the period 1992-2022]{Delinquency rate on credit card loans for the period 1992-2022{\citep{fedgraph_delinquency_history}}.}
	\label{fig:fredgraph}
\end{figure}

Existing models can be used to manage risk. However, developing models that perform better than those in use is feasible.

\subsection{Aim \& Approach}
The objective of this project was to explore different machine learning algorithms \& deep learning architectures on the American Express default prediction dataset\citep{amex-default-prediction-dataset} to predict if a customer will default on the payment in the future. The project work started by developing a model using classic machine learning algorithm \acf{SVM} followed by creating multiple models using Random Forest Classifier \& \acf{GBDT} algorithms. \acf{ANN}, \acf{GRU} \& Custom ensemble model created by model combining \acs{ANN}, \acs{GRU} and \acs{GBDT} were created as part of exploring deep learning architectures. Finally created a lean model, using less features \& optimzed parameters using \acs{GBDT} which provided comparable performances to the previously explored models.

\subsection{Structure of Report}
The remainder of the report is structured as follows: in Section \ref{sec:background_knowledge} background information on different machine learning \& deep learning algorithms along with metrics explanation is provided. Then in Section \ref{sec:literature_review} a literature review related to the credit card default prediction research is given. Section \ref{sec:materials} \& \ref{sec:methodology} provides detailed explanations on the dataset, tools \& software used in the project, and methodology followed for creating the models. Model evaluation results and the comparison is given in Section \ref{sec:results_discussions}. Finally Section \ref{sec:conclusion_summary} discusses the conclusion of the project. 

\vfill
\clearpage
\section{Background Knowledge} \label{sec:background_knowledge}
This section provides the reader with the required background information on the machine learning algorithms, deep learning architectures \& metrics.

\subsection{\acf{SVM}}
\subsection{Decision Tree}
\subsection{Ensemble Models}
\subsection{\acf{GBDT}}
\subsection{\acf{XGBoost}}
\subsection{\acf{LGBM}}
\subsection{\acf{ANN}}
\subsection{\acf{GRU}}
\subsection{Feature Selection - Select From Model}
\subsection{Ordinal Encoder}
\subsection{Class Imbalance}
\subsection{Data Oversampling}
\subsubsection{\acf{SMOTE}}
\subsubsection{KMeans \acs{SMOTE}}
\subsection{Metrics}
\subsubsection{Accuracy}
\subsubsection{F1-Score}
\subsubsection{Recall}
\subsection{\acf{CV}}
\subsection{Bias \& Variance }
\subsection{Hyper Parameter Tuning}
\subsubsection{Grid Search CV}
\subsection{Stochastic Gradient Descent}
\subsection{File Format}
\subsubsection{Parquet}
\subsection{Summary}

\vfill
\clearpage
\section{Literature Review}\label{sec:literature_review}
asfsf
\subsection{Summary}
dfsadfsd

\vfill
\clearpage
\section{Materials}\label{sec:materials}

\subsection{Primary Dataset}
The primary dataset contains 190 aggregated profile features of 458913 American Express customers at each statement date for 13 months. Features are anonymized and normalized, and fall into the following general categories:

\begin{itemize}
	\item D\_* = Delinquency variables
	\item S\_* = Spend variables
	\item P\_* = Payment variables
	\item B\_* = Balance variables
	\item R\_* = Risk variables	
\end{itemize}

This dataset\citep{amex-default-prediction-dataset} was released as part of the "American Express - Default Prediction" hosted in Kaggle by the American Express team.

\subsection{Secondary Dataset}
The secondary dataset was derived from primary dataset by applying the below mathematical aggregate operations to the numerical features.
\begin{itemize}
	\item Minimum 
	\item Maximum
	\item Mean
	\item Last Value
	\item Standard Deviation
\end{itemize}

Aggregate for the categorical features were taken by the applying below operations.
\begin{itemize}
	\item Last Value
	\item Count
	\item Unique Value Count
\end{itemize}

The secondary dataset contains 920 features and 458913 records.

\subsection{Tools \& Software}
The primary programming language used for the implementation of this project is Python version 3.7. Data analysis and manipulation is done using Pandas(1.3.5), seaborn(0.11.2) \& Dask(2.12.0) packages. Scikit Learn(1.0.2) package is used for create, train \& evaluate machine learning models. \acs{ANN} \& \acs{GRU} models were created using Tensorflow (2.8.2).Google colab was used to train the model in cloud and Github was used as the version control \& project management software.

\vfill
\clearpage
\section{Methodology}\label{sec:methodology}
\subsection{Introduction}
This section will first provide a brief overview of the overall strategy of the experiments performed as part of this project followed by  providing detailed explanation on the data preprocessing techniques used. Then in subsequent sections each experiment/model will be presented along with the model specific explanations \& details.

\subsection{Overview of Methodology Followed}
Figure \ref{fig:methodology} represents a overview of methodology in general followed for conducting experiments. The dataset was first split into chunks and stored in different files in parquet format to optimize the memory usage. Then, dataset was preprocessed to remove invalid values and encode categorical text variables to numerical values. Followed by data pre-processing, the dataset was split into Training \& Test set, this ensures that none of the entries in test set will have an influence in model training and model selection process. Then the dataset was enhanced using oversampling techniques to resolve the class imbalance issue;in addition, feature selection techniques were used to eliminate the features from the dataset which were less important and hence contribute very little to model. 
\begin{figure}[ht]
	\centering
	\includegraphics[width=1.0\textwidth, height=0.5\textheight]{methodology}
	\caption[Methodology]{Methodology followed for the experiments}
	\label{fig:methodology}
\end{figure}

After the feature selection, the model was created and then passed through a Hyperparameter tuning pipeline which helps to find the best parameters for the model which would give highest cross validation score. Finally the entire training dataset was trained on the best model found using hyperparameter tuning and the model was evaluated using the test set set aside at the beginning of the experiment.

\subsection{Data Preprocessing} 
This section discusses the common preprocessing techniques used in all experiments conducted as part of the project. The model specific data preporcessing techniques used will be discussed in respective sections describing the model.

\subsubsection{Default Values}
NaN \& NULL values in the dataset was replaced by Zero and if a column contains all values same, it was removed from the dataset. -1 was used as the default value for the categorical variables. The categorical variables were encoded using Ordinal Encoder before passing to the model training pipeline.

\subsubsection{Normalization}
The primary dataset from American Express is already normalized and all the values lies between zero to ten, hence none of the data normalization techniques were used to preporcess the data.
\subsubsection{Handling Memory Issue}
Google Colab provides 24 GB of \acf{RAM} in the  virtual environment, though the Primary Dataset is 16 GB,  the pandas library was unable to load the complete data into memory due to memory leakage issue in the framework. Dask library, which uses multiple Pandas dataframe under the hood, was used to overcome the memory. Dataframe API in Dask library splits the dataset into multiple chunks and loads each chunk on a need basis only ( Lazy Loading), this ensured that the complete 16 GB dataset could be loaded even at a low memory of 4GB.

Additionally, the primary dataset was loaded using Dask framework and split the dataset month wise, ie one file for each month. The month wise files were saved in parquet format which helped to reduce the total size of the dataset from 16GB to 7GB. Similarly the primary dataset was also split customer wise, ie 1-50000 customers data in one file, 50001-100000 customers data in second file etc. These files were later used to build the Secondary Dataset.

\subsection{Model 1 - \acf{SVM}}
The \acs{SVM} model was created with parameters Regularziation Term = L2 Norm(Squared Error Loss), Alpha = 0.0001, Loss='hinge'(soft-margin), tolerance=0.001. The primary dataset was used to train the model and the model converged after 28 iterations. Early stopping was used to prevent overfitting of the model and 10\% of the data from training set used as the validation set. Stochastic gradient descent was used to optimize the objective function, this ensured that even though the dataset contains millions of records, the training is able to proceed and finish in reasonable time. 
\subsection{Model 2 - Random Forest Classifier}
The Random Forest Classifier model uses 100 Decision Trees trained in parallel on the primary dataset. Each decision tree uses a different subset of Primary Dataset with maximum number of records in a database set to 600,000. Gini impurity metric is used to measure the quality of the split while building decision tree. Finally the model predicts the target variable by taking mean of all the predictions from the 100 individual decision trees.
\subsection{Model 3 - \acf{GBDT}}

\subsection{Model 4 - \acf{XGBoost}}
\subsection{Model 5 - \acf{LGBM}}
\subsection{Model 6 - \acf{LGBM + Data Oversampling}}
\subsection{Model 7 - \acf{ANN}}
\subsection{Model 8 - \acf{GRU}}
\subsection{Model 9 - Ensemble Stacking Model using \acs{ANN} + \acs{GRU} + \acs{GBDT}}
\subsection{Model 10 - Lean \acs{LGBM} Model}

\subsection{Summary}
dfsadfsd

\vfill
\clearpage
\section{Results \& Discussions}\label{sec:results_discussions}
asfsf
\subsection{Summary}
dfsadfsd

\vfill
\clearpage
\section{Conclusion \& Summary}\label{sec:conclusion_summary}
asfsf
\subsection{Summary}
dfsadfsd

\vfill
\clearpage

\lhead{}\chead{MSc. Project Report :: \nouppercase{\leftmark}}\rhead{}
\phantomsection
\addcontentsline{toc}{section}{References}
\bibliographystyle{agsm} 
\bibliography{mybib}

\clearpage

\section{Appendix One: Code}

\subsection{Directory Structure} 

\subsection{Running the Provided Code}

\clearpage
\end{document}
